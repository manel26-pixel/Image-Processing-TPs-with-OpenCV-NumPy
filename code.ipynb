{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenCV + NumPy Cheatsheet\n",
    "\n",
    "---\n",
    "\n",
    "## 🗾️ 1. Image I/O & Properties\n",
    "\n",
    "| Task | Code / Explanation |\n",
    "|------|--------------------|\n",
    "| Read a color image | `img = cv2.imread(image_path)` |\n",
    "| Read grayscale image | `img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)` |\n",
    "| Get image dimensions | `h, w, c = img.shape` (height, width, channels) |\n",
    "| Check image type | `img.dtype` |\n",
    "| Convert to float | `img = img.astype(np.float32)` |\n",
    "\n",
    "---\n",
    "\n",
    "## 🎨 2. Image Initialization\n",
    "\n",
    "| Task | Code |\n",
    "|------|------|\n",
    "| Create empty (zeros) image | `img = np.zeros((h, w, c), np.uint8)` |\n",
    "| Clone shape & type | `img2 = np.zeros_like(img)` |\n",
    "| Create image filled with 1s | `img = np.ones((h, w), np.float32)` |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔀 3. Image Manipulation\n",
    "\n",
    "| Task | Code |\n",
    "|------|------|\n",
    "| Invert image | `img_inv = 255 - img` |\n",
    "| Normalize values to [0, 1] | `img = img / 255.0` |\n",
    "| Custom normalization | `img[y,x] = ((img[y,x] - min) / (max - min)) * 255` |\n",
    "| Resize image | `resized = cv2.resize(img, (width, height))` |\n",
    "| Concatenate images | `cv2.hconcat([img1, img2])` (horizontal) |\n",
    "\n",
    "---\n",
    "\n",
    "## 🖛️ 4. Filtering & Convolution\n",
    "\n",
    "| Task | Code |\n",
    "|------|------|\n",
    "| 2D Convolution | `cv2.filter2D(img, -1, kernel)` |\n",
    "| Normalize result | `cv2.normalize(imgRes, imgRes, 0, 255, cv2.NORM_MINMAX)` |\n",
    "\n",
    "---\n",
    "\n",
    "## 🎺 5. Histogram & Plotting (Matplotlib)\n",
    "\n",
    "| Task | Code |\n",
    "|------|------|\n",
    "| Init histogram | `histo = np.zeros((256, 1), np.uint16)` |\n",
    "| Calculate histogram | `cv2.calcHist([img], [0], None, [256], [0,256])` |\n",
    "| Plot histogram | `plt.plot(histo); plt.xlim([0, 255]); plt.show()` |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔱 6. Trackbars (Interactive Sliders)\n",
    "\n",
    "| Task | Code |\n",
    "|------|------|\n",
    "| Create trackbar | `cv2.createTrackbar(\"thresh\", \"window\", 0, 255, callback_fn)` |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧬 7. Morphological Operations\n",
    "\n",
    "| Task | Code |\n",
    "|------|------|\n",
    "| Thresholding | `cv2.threshold(img, 128, 255, cv2.THRESH_BINARY, img)` |\n",
    "| Erode | `cv2.erode(img, kernel)` |\n",
    "| Dilate | `cv2.dilate(img, kernel)` |\n",
    "| MorphologyEx (e.g., Gradient) | `cv2.morphologyEx(img, cv2.MORPH_GRADIENT, kernel)` |\n",
    "| Kernel structure | `cv2.getStructuringElement(cv2.MORPH_CROSS, (size, size))` |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 8. Gradient & Edge Detection\n",
    "\n",
    "| Task | Code |\n",
    "|------|------|\n",
    "| Compute gradients | `grad_x = img[:, :-1] - img[:, 1:]` |\n",
    "| Pad arrays | `grad_x = np.pad(grad_x, ((0,0),(0,1)), mode='constant')` |\n",
    "| Gradient magnitude | `grad = np.sqrt(grad_x**2 + grad_y**2)` |\n",
    "\n",
    "---\n",
    "\n",
    "## 🎥 9. Video & Camera Input\n",
    "\n",
    "| Task | Code |\n",
    "|------|------|\n",
    "| From webcam | `cv2.VideoCapture(0)` |\n",
    "| From file | `cv2.VideoCapture('video.avi')` |\n",
    "| From phone | `cv2.VideoCapture(\"http://IP:PORT/video\")` |\n",
    "| Get frame size | `w = int(cap.get(3)), h = int(cap.get(4))` |\n",
    "| Read frame | `ret, frame = cap.read()` |\n",
    "| Flip frame | `cv2.flip(frame, 1, frame)` |\n",
    "\n",
    "---\n",
    "\n",
    "## 📀 10. Video Output (Recording)\n",
    "\n",
    "| Task | Code |\n",
    "|------|------|\n",
    "| Define codec | `fourcc = cv2.VideoWriter_fourcc('X','V','I','D')` |\n",
    "| Init writer | `out = cv2.VideoWriter('file.avi', fourcc, 30, (w, h))` |\n",
    "| Write frame | `out.write(frame)` |\n",
    "| Release | `out.release()` |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔴 11. Image Color Spaces\n",
    "\n",
    "| Task | Code |\n",
    "|------|------|\n",
    "| Split BGR | `img_b[:,:,0], img_g[:,:,1], img_r[:,:,2] = img[:,:,0], img[:,:,1], img[:,:,2]` |\n",
    "| RGB to gray (manual) | `gray = (b + g + r) / 3` or use weights |\n",
    "| BGR to HSV | `img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HLS)` |\n",
    "| BGR to Grayscale | `cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)` |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔄 12. Utility / Control Flow\n",
    "\n",
    "| Task | Code |\n",
    "|------|------|\n",
    "| Wait for key | `cv2.waitKey(0)` |\n",
    "| Destroy all windows | `cv2.destroyAllWindows()` |\n",
    "| Quit condition | `if cv2.waitKey(20) & 0xFF == ord('q'):` |\n",
    "| Random point | `x, y = randrange(w), randrange(h)` |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 13. Data Types in NumPy\n",
    "\n",
    "| Type | Range |\n",
    "|------|--------|\n",
    "| `uint8` | [0, 255] |\n",
    "| `int8` | [-128, 127] |\n",
    "| `uint16` | [0, 65535] |\n",
    "| `int16` | [-32768, 32767] |\n",
    "| `uint32` | [0, 4294967295] |\n",
    "| `int32` | [-2147483648, 2147483647] |\n",
    "| `float32` | ~ [1.2e-38, 3.4e+38] |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' TP 1 – Negative Transformation of an Image '''\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the image from file\n",
    "image_path = 'sadcat.jpeg'\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Check if the image was loaded successfully\n",
    "if image is None:\n",
    "    print(\"Error: The image couldn't be loaded. Please check the file path.\")\n",
    "    exit(0)\n",
    "else:\n",
    "    # Get image dimensions (height, width, channels)\n",
    "    h, w, c = image.shape\n",
    "\n",
    "    # Create a float32 image with the same shape as the original\n",
    "    imgRes = np.zeros(image.shape, np.float32)\n",
    "\n",
    "    # Compute the negative of the image using vectorized operation\n",
    "    # Subtract each pixel value from 255 (max intensity) to invert the image\n",
    "    imgRes[:] = 255 - image[:]\n",
    "\n",
    "    # Normalize the result to [0, 1] range (optional step depending on use case)\n",
    "    imgRes = imgRes / 255.\n",
    "\n",
    "    # Display the original and the resulting images\n",
    "    cv2.imshow(\"Original Image\", image)\n",
    "    cv2.imshow(\"Negative Image\", imgRes)\n",
    "\n",
    "    # Wait for a key press before closing the image windows\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' TP 2 – Manual Normalization of a Grayscale Image '''\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the image in grayscale mode\n",
    "image_path = 'sadcat.jpeg'\n",
    "img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Simulate an image with low brightness by dividing pixel values by 2\n",
    "img[:] = img[:] / 2\n",
    "\n",
    "# Uncomment to save this darker version if needed\n",
    "# cv2.imwrite(\"img2.jfif\", img)\n",
    "\n",
    "# Check if the image was loaded successfully\n",
    "if img is None:\n",
    "    print(\"Error: The image couldn't be loaded. Please check the file path.\")\n",
    "    exit(0)\n",
    "\n",
    "# Create an empty image to store the normalized result\n",
    "imgNorm = np.zeros(img.shape, np.uint8)\n",
    "\n",
    "# Get the image dimensions\n",
    "h, w = img.shape\n",
    "\n",
    "# Initialize min and max values for normalization\n",
    "min_val = 255\n",
    "max_val = 0\n",
    "\n",
    "# Find the minimum and maximum pixel values in the image\n",
    "for y in range(h):\n",
    "    for x in range(w):\n",
    "        if img[y, x] > max_val:\n",
    "            max_val = img[y, x]\n",
    "        if img[y, x] < min_val:\n",
    "            min_val = img[y, x]\n",
    "\n",
    "# Normalize the pixel values to span the full range [0, 255]\n",
    "for y in range(h):\n",
    "    for x in range(w):\n",
    "        imgNorm[y, x] = ((img[y, x] - min_val) / (max_val - min_val)) * 255\n",
    "\n",
    "# Print the original min and max values (for verification)\n",
    "print(\"Min pixel value:\", min_val)\n",
    "print(\"Max pixel value:\", max_val)\n",
    "\n",
    "# Display the original (darkened) and normalized images\n",
    "cv2.imshow(\"Original Image (Darkened)\", img)\n",
    "cv2.imshow(\"Normalized Image\", imgNorm)\n",
    "\n",
    "# Wait for a key press and then close all windows\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty histogram array of size 256 (for 256 grayscale levels)\n",
    "# np.uint16 is used to handle large counts, depending on the image size\n",
    "histo1 = np.zeros((256, 1), np.uint16)\n",
    "\n",
    "# Manually compute the histogram of the original (darkened) image\n",
    "for y in range(h):\n",
    "    for x in range(w):\n",
    "        histo1[img[y, x], 0] += 1\n",
    "\n",
    "# Use OpenCV’s built-in function to calculate histogram of the normalized image\n",
    "histo2 = cv2.calcHist([imgNorm], [0], None, [256], [0, 255])\n",
    "\n",
    "# Plot both histograms using Matplotlib\n",
    "plt.figure()\n",
    "plt.title(\"Normalized Image Histogram\")\n",
    "plt.xlabel(\"Gray Level\")\n",
    "plt.ylabel(\"Number of Pixels\")\n",
    "\n",
    "# Plot the manually computed histogram for the original image\n",
    "plt.plot(histo1, label=\"Original Image (Manual Histogram)\")\n",
    "\n",
    "# Plot the OpenCV-computed histogram for the normalized image\n",
    "plt.plot(histo2, label=\"Normalized Image (cv2.calcHist)\", linestyle='dashed')\n",
    "\n",
    "# Limit x-axis to valid grayscale range\n",
    "plt.xlim([0, 255])\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HISTOGRAM EXAMPLE 2 – Visualizing Horizontal and Vertical Projections ===\n",
    "\n",
    "import cv2 \n",
    "import numpy as np \n",
    "\n",
    "# Load the grayscale image\n",
    "img = cv2.imread('sadcat.jpeg', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Apply binary thresholding: pixels > 130 become 255, others become 0\n",
    "cv2.threshold(img, 130, 255, cv2.THRESH_BINARY, img)\n",
    "\n",
    "# Get image dimensions\n",
    "h, w = img.shape \n",
    "\n",
    "# Initialize arrays to count black pixels per row and column\n",
    "lignes = np.zeros((h), np.uint16)  # Horizontal projection (per row)\n",
    "cols = np.zeros((w), np.uint16)    # Vertical projection (per column)\n",
    "\n",
    "# Count black pixels (value = 0) in each row and column\n",
    "for i in range(h):\n",
    "    for j in range(w):\n",
    "        if img[i, j] == 0:\n",
    "            lignes[i] += 1\n",
    "            cols[j] += 1\n",
    "\n",
    "# Create white images to visualize horizontal and vertical projections\n",
    "imgLignes = np.full(img.shape, 255, dtype=np.uint8)\n",
    "imgCols = np.full(img.shape, 255, dtype=np.uint8)\n",
    "\n",
    "# Draw black lines in each row based on the count of black pixels\n",
    "for i in range(h):\n",
    "    for j in range(lignes[i]):\n",
    "        imgLignes[i, j] = 0\n",
    "\n",
    "# Draw black lines in each column based on the count of black pixels\n",
    "for j in range(w):\n",
    "    for i in range(cols[j]):\n",
    "        imgCols[i, j] = 0\n",
    "\n",
    "# Display the original and the histogram visualizations side by side\n",
    "cv2.imshow(\"Source Image\", img)\n",
    "cv2.imshow(\"Horizontal Projection (Row Histogram)\", cv2.hconcat([img, imgLignes]))\n",
    "cv2.imshow(\"Vertical Projection (Column Histogram)\", cv2.hconcat([img, imgCols]))\n",
    "\n",
    "# Wait for a key press and close all windows\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange \n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# === EXERCISE 3 – Random Point Movement on a Binary Image ===\n",
    "\n",
    "# Function to create an image with all white pixels and a single black pixel at a random location\n",
    "def createImgWithPointRand(h, w):\n",
    "    img = np.ones((h, w), np.float32)  # Create a white image (all values = 1.0)\n",
    "    randPointY, randPointX = randrange(h), randrange(w)  # Choose a random point\n",
    "    img[randPointY, randPointX] = 0  # Set that random point to black (0.0)\n",
    "    return img\n",
    "\n",
    "# Function to find the position of the black pixel (value = 0) in the image\n",
    "def findBlackPixel(img):\n",
    "    h, w = img.shape\n",
    "    for y in range(h):\n",
    "        for x in range(w):\n",
    "            if img[y, x] == 0:\n",
    "                return (y, x)\n",
    "\n",
    "# === Image configuration ===\n",
    "heightImg = 200\n",
    "widthImg = 400\n",
    "step = 3  # Number of pixels to move each time\n",
    "\n",
    "# Create the initial image and locate the black pixel\n",
    "img = createImgWithPointRand(heightImg, widthImg)\n",
    "(py, px) = findBlackPixel(img)\n",
    "\n",
    "# Initialize input key\n",
    "q = 'a'\n",
    "\n",
    "# === Main interaction loop ===\n",
    "while True:\n",
    "    # Down arrow key (ASCII 50): move the black pixel downward\n",
    "    if q == 50 and py + step < heightImg:\n",
    "        img[py, px] = 1\n",
    "        img[py + step, px] = 0\n",
    "        py = py + step\n",
    "\n",
    "    # Up arrow key (ASCII 56): move the black pixel upward\n",
    "    if q == 56 and py - step >= 0:\n",
    "        img[py, px] = 1\n",
    "        img[py - step, px] = 0\n",
    "        py = py - step\n",
    "\n",
    "    # Left arrow key (ASCII 52): move the black pixel to the left\n",
    "    if q == 52 and px - step >= 0:\n",
    "        img[py, px] = 1\n",
    "        img[py, px - step] = 0\n",
    "        px = px - step\n",
    "\n",
    "    # Right arrow key (ASCII 54): move the black pixel to the right\n",
    "    if q == 54 and px + step < widthImg:\n",
    "        img[py, px] = 1\n",
    "        img[py, px + step] = 0\n",
    "        px = px + step\n",
    "\n",
    "    # Display the updated image\n",
    "    cv2.imshow('Image', img)\n",
    "\n",
    "    # Wait for a key press and get its ASCII code (only the last byte)\n",
    "    q = cv2.waitKey(0) & 0xFF\n",
    "\n",
    "    # Press '0' to quit the loop (ASCII code of '0' is 48)\n",
    "    if ord('0') == q:\n",
    "        break\n",
    "\n",
    "# Close all OpenCV windows\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' TP 3 – Smoothing Filters: Mean and Median '''\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.lib.shape_base import vsplit\n",
    "\n",
    "# Load the image in grayscale mode\n",
    "image_path = 'sadcat.jpeg'\n",
    "image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Size of the neighborhood (must be odd)\n",
    "vois = 3  # Kernel size (3x3 neighborhood)\n",
    "\n",
    "# === Mean Filter ===\n",
    "def filtreMoy(img):\n",
    "    h, w = img.shape\n",
    "    imgMoy = np.zeros(img.shape, img.dtype)\n",
    "\n",
    "    for y in range(h):\n",
    "        for x in range(w):\n",
    "            # Skip pixels near the border to avoid out-of-bounds access\n",
    "            if y < vois/2 or y > (h - vois/2) or x < vois/2 or x > (w - vois/2):\n",
    "                imgMoy[y, x] = img[y, x]\n",
    "            else:\n",
    "                m = int(vois / 2)\n",
    "                # Extract the neighborhood (voisinage)\n",
    "                imgVois = img[y - m:y + m + 1, x - m:x + m + 1]\n",
    "                # Compute the mean and assign it to the current pixel\n",
    "                imgMoy[y, x] = np.mean(imgVois)\n",
    "    return imgMoy\n",
    "\n",
    "# === Median Filter ===\n",
    "def filtreMedian(img):\n",
    "    h, w = img.shape\n",
    "    imgMed = np.zeros(img.shape, img.dtype)\n",
    "\n",
    "    for y in range(h):\n",
    "        for x in range(w):\n",
    "            # Skip border pixels\n",
    "            if y < vois/2 or y > (h - vois/2) or x < vois/2 or x > (w - vois/2):\n",
    "                imgMed[y, x] = img[y, x]\n",
    "            else:\n",
    "                m = int(vois / 2)\n",
    "                # Extract the neighborhood\n",
    "                imgVois = img[y - m:y + m + 1, x - m:x + m + 1]\n",
    "                # Compute the median and assign it to the current pixel\n",
    "                imgMed[y, x] = np.median(imgVois)\n",
    "    return imgMed\n",
    "\n",
    "# Apply filters to the image\n",
    "imgMoy = filtreMoy(image)\n",
    "imgMed = filtreMedian(image)\n",
    "\n",
    "# Display original and filtered images\n",
    "cv2.imshow(\"Original Image\", image)\n",
    "cv2.imshow(\"Mean Filtered Image\", imgMoy)\n",
    "cv2.imshow(\"Median Filtered Image\", imgMed)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' TP 4 – Interactive Image Thresholding with Trackbars '''\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# === Load the grayscale image ===\n",
    "image_path = 'sadcat.jpeg'\n",
    "img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Initial threshold value and type\n",
    "th = 0           # Threshold level (0 to 255)\n",
    "type_th = 0      # Thresholding method (OpenCV types 0 to 4)\n",
    "\n",
    "# === Function to apply thresholding and display the result ===\n",
    "def afficher():\n",
    "    # Create an output image with the same shape and type as input\n",
    "    imgRes = np.zeros_like(img)\n",
    "\n",
    "    # Apply thresholding using OpenCV's built-in function\n",
    "    # Parameters:\n",
    "    # - img: input grayscale image\n",
    "    # - th: threshold value\n",
    "    # - 255: maximum value to use\n",
    "    # - type_th: thresholding type (e.g., binary, inverse, trunc, etc.)\n",
    "    # - imgRes: destination image\n",
    "    cv2.threshold(img, th, 255, type_th, imgRes)\n",
    "\n",
    "    # Show the result in a window\n",
    "    cv2.imshow('img', imgRes)\n",
    "\n",
    "# === Callback function: updates threshold value ===\n",
    "def change_th(x):\n",
    "    global th\n",
    "    th = x\n",
    "    afficher()\n",
    "\n",
    "# === Callback function: updates thresholding method ===\n",
    "def change_type(x):\n",
    "    global type_th\n",
    "    type_th = x\n",
    "    afficher()\n",
    "\n",
    "# Initial display of the image\n",
    "afficher()\n",
    "\n",
    "# === Create trackbars ===\n",
    "\n",
    "# Threshold value trackbar: lets user choose a threshold between 0 and 255\n",
    "cv2.createTrackbar(\"thresh\", \"img\", 0, 256, change_th)\n",
    "\n",
    "# Thresholding type trackbar: choose among 5 OpenCV thresholding modes\n",
    "# 0: Binary\n",
    "# 1: Binary Inverse\n",
    "# 2: Truncate\n",
    "# 3: To Zero\n",
    "# 4: To Zero Inverse\n",
    "cv2.createTrackbar(\"type\", \"img\", 0, 4, change_type)\n",
    "\n",
    "# Wait for a key press before closing the windows\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Gradient-Based Edge Detection with Interactive Thresholding '''\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# === Load the image in grayscale ===\n",
    "image_path = 'sadcat.jpeg'\n",
    "img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Initial threshold value for edge detection\n",
    "th = 0\n",
    "\n",
    "# === Function to compute and display the edge map ===\n",
    "def afficher():\n",
    "    # Create an empty image to store the contours (edges)\n",
    "    imgContour = np.zeros_like(img)  # Same size and type as the original\n",
    "\n",
    "    # Compute the gradient in the X direction (horizontal difference)\n",
    "    grad_x = img[:, :img.shape[1] - 1] - img[:, 1:]\n",
    "\n",
    "    # Compute the gradient in the Y direction (vertical difference)\n",
    "    grad_y = img[:img.shape[0] - 1, :] - img[1:, :]\n",
    "\n",
    "    # Pad gradients to match the original image size\n",
    "    grad_x = np.pad(grad_x, ((0, 0), (0, 1)), mode='constant')  # Pad last column\n",
    "    grad_y = np.pad(grad_y, ((0, 1), (0, 0)), mode='constant')  # Pad last row\n",
    "\n",
    "    # Compute the gradient magnitude (Euclidean norm)\n",
    "    grad = np.sqrt(grad_x**2 + grad_y**2)\n",
    "\n",
    "    # Apply threshold: highlight edges where gradient magnitude > th\n",
    "    imgContour[grad > th] = 255  # Strong edge\n",
    "    imgContour[grad <= th] = 0   # Weak or no edge\n",
    "\n",
    "    # Display the resulting edge image\n",
    "    cv2.imshow('img', imgContour)\n",
    "\n",
    "# === Callback function to update the threshold from the trackbar ===\n",
    "def change_th(x):\n",
    "    global th\n",
    "    th = x\n",
    "    afficher()\n",
    "\n",
    "# Create the display window\n",
    "cv2.namedWindow(\"img\")\n",
    "\n",
    "# Create a trackbar to adjust the threshold value\n",
    "cv2.createTrackbar(\"thresh\", \"img\", 0, 256, change_th)\n",
    "\n",
    "# Initial display\n",
    "afficher()\n",
    "\n",
    "# Wait for key press before closing\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Section 1: Gaussian Blur (Smoothing) ===\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load grayscale image\n",
    "image_path = 'sadcat.jpeg'\n",
    "img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Define a 3x3 Gaussian kernel\n",
    "kernel = np.array([[1, 2, 1],\n",
    "                   [2, 4, 2],\n",
    "                   [1, 2, 1]]) / 16\n",
    "\n",
    "# Apply the Gaussian filter using convolution\n",
    "imgRes = cv2.filter2D(img, -1, kernel)\n",
    "\n",
    "# Normalize the result to the 0–255 range\n",
    "cv2.normalize(imgRes, imgRes, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "# Display the original and filtered image\n",
    "cv2.imshow(\"Original Image\", img)\n",
    "cv2.imshow(\"Gaussian Blurred\", imgRes)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Section 2: Laplacian Filtering + Edge Enhancement ===\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load image in grayscale\n",
    "image_path = 'sadcat.jpeg'\n",
    "img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Define a Laplacian kernel (edge detection)\n",
    "kernel = np.array([[0, -1, 0],\n",
    "                   [-1, 4, -1],\n",
    "                   [0, -1, 0]])\n",
    "\n",
    "# Apply convolution\n",
    "imgRes = cv2.filter2D(img.astype(np.int16), -1, kernel)\n",
    "\n",
    "# Add the result to the original image to enhance edges\n",
    "imgRes = img + imgRes\n",
    "\n",
    "# Normalize and display\n",
    "cv2.normalize(imgRes, imgRes, 0, 255, cv2.NORM_MINMAX)\n",
    "cv2.imshow(\"Original Image\", img)\n",
    "cv2.imshow(\"Laplacian Enhanced\", imgRes.astype(np.uint8))\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Section 3: Custom Gradient Kernel + Enhancement ===\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load grayscale image\n",
    "image_path = 'sadcat.jpeg'\n",
    "img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Define a simple custom gradient kernel\n",
    "kernel = np.array([[0, -1, 0],\n",
    "                   [-1, 1, -1],\n",
    "                   [0, -1, 0]])\n",
    "\n",
    "# Apply filter and enhance result\n",
    "imgRes = cv2.filter2D(img.astype(np.int16), -1, kernel)\n",
    "imgRes = img + imgRes\n",
    "\n",
    "# Normalize and display\n",
    "cv2.normalize(imgRes, imgRes, 0, 255, cv2.NORM_MINMAX)\n",
    "cv2.imshow(\"Original Image\", img)\n",
    "cv2.imshow(\"Gradient Enhanced\", imgRes.astype(np.uint8))\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Section 4: Sharpening with Laplacian Kernel ===\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load grayscale image\n",
    "image_path = 'sadcat.jpeg'\n",
    "img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Sharpening kernel (Laplacian + identity)\n",
    "kernel = np.array([[0, -1, 0],\n",
    "                   [-1, 5, -1],\n",
    "                   [0, -1, 0]])\n",
    "\n",
    "# Apply sharpening filter\n",
    "imgRes = cv2.filter2D(img.astype(np.int16), -1, kernel)\n",
    "\n",
    "# Normalize and display\n",
    "cv2.normalize(imgRes, imgRes, 0, 255, cv2.NORM_MINMAX)\n",
    "cv2.imshow(\"Original Image\", img)\n",
    "cv2.imshow(\"Sharpened\", imgRes.astype(np.uint8))\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Section 5: Gaussian Kernel Generator and Display ===\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Function to compute the Gaussian value for coordinates (x, y)\n",
    "def gauss(x, y, sigma):\n",
    "    part1 = 1 / (2 * math.pi * sigma**2)\n",
    "    part2 = -((x**2 + y**2) / (2 * sigma**2))\n",
    "    return part1 * math.exp(part2)\n",
    "\n",
    "# Print and view Gaussian kernel\n",
    "def print_gauss(sigma=1.4, vois_mat=5):\n",
    "    vois = vois_mat // 2\n",
    "    som = 0.0\n",
    "    for i in range(-vois, vois + 1):\n",
    "        for j in range(-vois, vois + 1):\n",
    "            val = round(gauss(i, j, sigma) * 185, 0)\n",
    "            print('{:02.2f}'.format(val), '\\t', end=\"\")\n",
    "            som += val\n",
    "        print('')\n",
    "    print('Sum =', som)\n",
    "\n",
    "print_gauss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Section 6: Return the Gaussian kernel as a NumPy matrix ===\n",
    "\n",
    "def check_gauss(mat):\n",
    "    for row in mat:\n",
    "        print(\" \".join(map(str, row)))\n",
    "\n",
    "def get_gauss(sigma=1.4, vois_mat=5):\n",
    "    mat_gauss = np.zeros((vois_mat, vois_mat), float)\n",
    "    vois = vois_mat // 2\n",
    "    som = 0.0\n",
    "    for i in range(-vois, vois + 1):\n",
    "        for j in range(-vois, vois + 1):\n",
    "            val = round(gauss(i, j, sigma) * 185, 0)\n",
    "            mat_gauss[i + vois][j + vois] = val\n",
    "            som += val\n",
    "    return som, mat_gauss\n",
    "\n",
    "som, mat_gauss = get_gauss()\n",
    "check_gauss(mat_gauss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Section 7: Apply Custom Gaussian Kernel ===\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load grayscale image\n",
    "image_path = 'sadcat.jpeg'\n",
    "img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Generate Gaussian kernel\n",
    "som, kernel = get_gauss()\n",
    "\n",
    "# Apply the kernel as a filter\n",
    "imgRes = cv2.filter2D(img.astype(np.int16), -1, kernel)\n",
    "\n",
    "# Normalize and display result\n",
    "cv2.normalize(imgRes, imgRes, 0, 255, cv2.NORM_MINMAX)\n",
    "cv2.imshow(\"Original Image\", img)\n",
    "cv2.imshow(\"Gaussian Filtered (Custom)\", imgRes.astype(np.uint8))\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' TP 6 – Morphological Operations with Trackbars (Erosion, Dilation, Gradient) '''\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# === Load the grayscale image ===\n",
    "image_path = 'sadcat.jpeg'\n",
    "img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Convert to binary image using a fixed threshold\n",
    "cv2.threshold(img, 128, 255, cv2.THRESH_BINARY, img)\n",
    "\n",
    "# Create a window for each operation\n",
    "cv2.namedWindow(\"Erosion\")\n",
    "cv2.namedWindow(\"Dilation\")\n",
    "cv2.namedWindow(\"Morph Gradient\")\n",
    "\n",
    "# === Initial sizes for structuring elements (adjusted by trackbars) ===\n",
    "sizeErode = 1\n",
    "sizeDilate = 1\n",
    "sizeMorph = 1\n",
    "\n",
    "# === Erosion Function ===\n",
    "def erode_func():\n",
    "    size = sizeErode * 2 + 1  # Ensure odd kernel size (e.g., 3, 5, 7...)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_CROSS, (size, size))\n",
    "    print(\"Erode kernel:\\n\", kernel)\n",
    "    img_erode = cv2.erode(img, kernel)\n",
    "    cv2.imshow(\"Erosion\", img_erode)\n",
    "\n",
    "# === Dilation Function ===\n",
    "def dilate_func():\n",
    "    size = sizeDilate * 2 + 1\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_CROSS, (size, size))\n",
    "    print(\"Dilate kernel:\\n\", kernel)\n",
    "    img_dilate = cv2.dilate(img, kernel)\n",
    "    cv2.imshow(\"Dilation\", img_dilate)\n",
    "\n",
    "# === Morphological Gradient Function ===\n",
    "def morph_func():\n",
    "    size = sizeMorph * 2 + 1\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_CROSS, (size, size))\n",
    "    print(\"Morph Gradient kernel:\\n\", kernel)\n",
    "    # Morph gradient = dilation - erosion\n",
    "    img_morph = cv2.morphologyEx(img, cv2.MORPH_GRADIENT, kernel)\n",
    "    cv2.imshow(\"Morph Gradient\", img_morph)\n",
    "\n",
    "# === Trackbar callbacks ===\n",
    "def changeErodeSize(x):\n",
    "    global sizeErode\n",
    "    sizeErode = x\n",
    "    erode_func()\n",
    "\n",
    "def changeDilateSize(x):\n",
    "    global sizeDilate\n",
    "    sizeDilate = x\n",
    "    dilate_func()\n",
    "\n",
    "def changeMorphSize(x):\n",
    "    global sizeMorph\n",
    "    sizeMorph = x\n",
    "    morph_func()\n",
    "\n",
    "# === Create trackbars to adjust structuring element size dynamically ===\n",
    "cv2.createTrackbar(\"Erode Size\", \"Erosion\", sizeErode, 17, changeErodeSize)\n",
    "cv2.createTrackbar(\"Dilate Size\", \"Dilation\", sizeDilate, 17, changeDilateSize)\n",
    "cv2.createTrackbar(\"Morph Size\", \"Morph Gradient\", sizeMorph, 17, changeMorphSize)\n",
    "\n",
    "# === Show the original binary image ===\n",
    "cv2.imshow(\"Original Binary\", img)\n",
    "\n",
    "# === Initial display ===\n",
    "erode_func()\n",
    "dilate_func()\n",
    "morph_func()\n",
    "\n",
    "# === Wait until key is pressed ===\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' TP 7 '''\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# === Read the original color image ===\n",
    "image_path = 'sadcat.jpeg'\n",
    "img = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "\n",
    "# Create empty images for each color channel with the same shape and type as original\n",
    "img_b = np.zeros(img.shape, img.dtype)\n",
    "img_g = np.zeros(img.shape, img.dtype)\n",
    "img_r = np.zeros(img.shape, img.dtype)\n",
    "\n",
    "# Get image dimensions\n",
    "h, w, c = img.shape\n",
    "\n",
    "'''\n",
    "# Method 1: Manual pixel-wise assignment (commented out for performance)\n",
    "for y in range(h):\n",
    "    for x in range(w):\n",
    "        img_b[y,x,0] = img[y,x,0]\n",
    "        img_g[y,x,1] = img[y,x,1]\n",
    "        img_r[y,x,2] = img[y,x,2]\n",
    "'''\n",
    "\n",
    "# Method 2: Efficient slicing to separate channels\n",
    "img_b[:, :, 0], img_g[:, :, 1], img_r[:, :, 2] = img[:, :, 0], img[:, :, 1], img[:, :, 2]\n",
    "\n",
    "'''\n",
    "# Compute grayscale manually using channel weighting:\n",
    "# Grayscale = 0.1*Blue + 0.6*Green + 0.3*Red\n",
    "# Using weights helps avoid truncation and keeps float precision\n",
    "'''\n",
    "\n",
    "# Basic grayscale computation using equal contribution from each channel\n",
    "img_gray = (np.float32(img_b[..., 0]) + \n",
    "            np.float32(img_g[..., 1]) + \n",
    "            np.float32(img_r[..., 2])) / 3\n",
    "\n",
    "# Optional: Normalize grayscale to [0, 1] for floating point visualization\n",
    "img_gray = img_gray / 255  # (or divide by 3*255 if weights used)\n",
    "\n",
    "# Convert BGR image to HLS color space (Hue, Lightness, Saturation)\n",
    "img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HLS)\n",
    "\n",
    "# Convert original image to float representation in range [0, 1]\n",
    "img_float = img / 255\n",
    "\n",
    "# === Display results ===\n",
    "cv2.imshow(\"image B\", img_b)\n",
    "cv2.imshow(\"image G\", img_g)\n",
    "cv2.imshow(\"image R\", img_r)\n",
    "cv2.imshow(\"image gray\", img_gray)\n",
    "cv2.imshow(\"image hsv\", img_hsv)\n",
    "cv2.imshow(\"image float\", img_float)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' TP 8 – Video Sampling and Recording '''\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time \n",
    "\n",
    "# === Capture source ===\n",
    "# Choose one of the following:\n",
    "# cap = cv2.VideoCapture(0)                       # 1. For webcam\n",
    "# cap = cv2.VideoCapture('output.avi')           # 2. From a saved video\n",
    "# url = \"http://192.168.226.189:8080/video\"      \n",
    "# cap = cv2.VideoCapture(url)                    # 3. IP camera (mobile phone via IP Webcam app)\n",
    "\n",
    "cap = cv2.VideoCapture(0)  # ← Example: using webcam\n",
    "\n",
    "# === Get frame dimensions ===\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# === Check if capture opened successfully ===\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open video source.\")\n",
    "    exit(0)\n",
    "\n",
    "# === Video writer (output file with XVID codec, 30 fps) ===\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('output2.avi', fourcc, 30, (frame_width, frame_height))\n",
    "\n",
    "# === Frame reading loop ===\n",
    "while cap.isOpened():\n",
    "\n",
    "    debut = time.time()  # Time before reading the frame\n",
    "\n",
    "    ret, frame = cap.read()  # Read a frame\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Error: Unable to read frame.\")\n",
    "        break\n",
    "\n",
    "    # Optional: Flip the frame (uncomment if needed)\n",
    "    # frame = cv2.flip(frame, 0)\n",
    "\n",
    "    out.write(frame)              # Save the frame to the output file\n",
    "    cv2.imshow(\"image\", frame)    # Display the frame\n",
    "\n",
    "    # Exit loop if 'q' is pressed\n",
    "    if cv2.waitKey(20) & 0xFF == ord('q'): \n",
    "        break\n",
    "\n",
    "    # Print time per frame and FPS\n",
    "    time_iter = time.time() - debut\n",
    "    print(\"time =\", round(time_iter, 4), \"s | fps =\", round(1. / time_iter, 2))\n",
    "\n",
    "# === Release resources ===\n",
    "out.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' TP 9 '''\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Uncomment this line to use your phone's IP webcam\n",
    "# url = \"http://192.168.226.189:8080/video\"\n",
    "# VideoCap = cv2.VideoCapture(url)\n",
    "\n",
    "# Use default webcam (computer camera)\n",
    "VideoCap = cv2.VideoCapture(0)\n",
    "\n",
    "# Define HSV color range for detection (example: blue object)\n",
    "lo = np.array([95, 80, 60])      # Lower HSV bound\n",
    "hi = np.array([115, 255, 255])   # Upper HSV bound\n",
    "\n",
    "# Optional: set up video recording\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('recorded_output.avi', fourcc, 30, (640, 480))\n",
    "\n",
    "# === Function to detect objects within HSV range ===\n",
    "def detect_inrange(image, min, max):\n",
    "    points = []\n",
    "    image = cv2.blur(image, (5, 5))  # Blur to reduce noise\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)  # Convert to HSV\n",
    "\n",
    "    # Create mask from HSV range\n",
    "    mask = cv2.inRange(image, lo, hi)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, None, iterations=2)  # Clean mask\n",
    "\n",
    "    # Find contours\n",
    "    elements = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[-2]\n",
    "    elements = sorted(elements, key=lambda x: cv2.contourArea(x), reverse=True)\n",
    "\n",
    "    for element in elements:\n",
    "        area = int(cv2.contourArea(element))\n",
    "        if min < area < max:\n",
    "            ((x, y), radius) = cv2.minEnclosingCircle(element)\n",
    "            points.append(np.array([int(x), int(y), int(radius), area]))\n",
    "\n",
    "    return image, mask, points\n",
    "\n",
    "# === Verify capture is working ===\n",
    "if not VideoCap.isOpened():\n",
    "    print(\"Error: Cannot open video stream.\")\n",
    "    exit(0)\n",
    "\n",
    "# === Main loop ===\n",
    "while True:\n",
    "    ret, frame = VideoCap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Cannot read frame.\")\n",
    "        break\n",
    "\n",
    "    frame = cv2.resize(frame, (640, 480))\n",
    "    cv2.flip(frame, 1, frame)  # Flip horizontally\n",
    "\n",
    "    image, mask, points = detect_inrange(frame, 1000, 3000)\n",
    "\n",
    "    # Debug point\n",
    "    cv2.circle(frame, (100, 100), 20, (0, 255, 0), 5)\n",
    "    print(image[100, 100])\n",
    "\n",
    "    # Draw circle and info if any point found\n",
    "    if len(points) > 0:\n",
    "        cv2.circle(frame, (points[0][0], points[0][1]), points[0][2], (0, 0, 255), 2)\n",
    "        cv2.putText(frame, str(points[0][3]), (points[0][0], points[0][1]),\n",
    "                    cv2.FONT_HERSHEY_COMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Display mask and frame\n",
    "    if mask is not None:\n",
    "        cv2.imshow(\"mask\", mask)\n",
    "\n",
    "    cv2.imshow(\"image\", frame)\n",
    "    out.write(frame)  # Save to video file\n",
    "\n",
    "    # Press 'q' to stop\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Cleanup\n",
    "VideoCap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' TP 10 : Kalman Filter Tracking '''\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class KalmanFilter:\n",
    "    def __init__(self, dt, point):\n",
    "        self.dt = dt  # Time step\n",
    "\n",
    "        # Initial state vector [x, y, vx, vy]\n",
    "        self.E = np.matrix([[point[0]], [point[1]], [0], [0]])\n",
    "\n",
    "        # State transition matrix (A)\n",
    "        self.A = np.matrix([\n",
    "            [1, 0, self.dt, 0],\n",
    "            [0, 1, 0, self.dt],\n",
    "            [0, 0, 1, 0],\n",
    "            [0, 0, 0, 1]\n",
    "        ])\n",
    "\n",
    "        # Observation matrix (we only observe position x, y)\n",
    "        self.H = np.matrix([\n",
    "            [1, 0, 0, 0],\n",
    "            [0, 1, 0, 0]\n",
    "        ])\n",
    "\n",
    "        # Process noise covariance (Q)\n",
    "        self.Q = np.matrix([\n",
    "            [1, 0, 0, 0],\n",
    "            [0, 1, 0, 0],\n",
    "            [0, 0, 1, 0],\n",
    "            [0, 0, 0, 1]\n",
    "        ])\n",
    "\n",
    "        # Measurement noise covariance (R)\n",
    "        self.R = np.matrix([\n",
    "            [1, 0],\n",
    "            [0, 1]\n",
    "        ])\n",
    "\n",
    "        # Initial estimation error covariance (P)\n",
    "        self.P = np.eye(4)\n",
    "\n",
    "    def predict(self):\n",
    "        # Predict the next state\n",
    "        self.E = self.A @ self.E\n",
    "\n",
    "        # Update error covariance\n",
    "        self.P = self.A @ self.P @ self.A.T + self.Q\n",
    "\n",
    "        return self.E\n",
    "\n",
    "    def update(self, z):\n",
    "        # Compute Kalman Gain\n",
    "        S = self.H @ self.P @ self.H.T + self.R\n",
    "        K = self.P @ self.H.T @ np.linalg.inv(S)\n",
    "\n",
    "        # Update estimate with measurement z\n",
    "        self.E = np.round(self.E + K @ (z - self.H @ self.E))\n",
    "\n",
    "        # Update error covariance\n",
    "        I = np.eye(self.H.shape[1])\n",
    "        self.P = (I - K @ self.H) @ self.P\n",
    "\n",
    "        return self.E\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' TP 11 : Face Detection with Kalman Filter '''\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# HSV range for optional object detection (e.g., blue object)\n",
    "lo = np.array([95, 100, 30])\n",
    "hi = np.array([125, 255, 255])\n",
    "\n",
    "# Function to detect objects in a specific HSV color range\n",
    "def detect_inrange(image, surfaceMin, surfaceMax):\n",
    "    points = []\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)  # Convert to HSV\n",
    "    image = cv2.blur(image, (5, 5))                 # Blur to reduce noise\n",
    "    mask = cv2.inRange(image, lo, hi)               # Threshold in range\n",
    "    mask = cv2.erode(mask, None, iterations=2)      # Morphological erosion\n",
    "    mask = cv2.dilate(mask, None, iterations=2)     # Morphological dilation\n",
    "\n",
    "    # Detect contours in the mask\n",
    "    elements = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[-2]\n",
    "    elements = sorted(elements, key=lambda x: cv2.contourArea(x), reverse=True)\n",
    "\n",
    "    for element in elements:\n",
    "        print('Surface :', cv2.contourArea(element))\n",
    "        if surfaceMin < cv2.contourArea(element) < surfaceMax:\n",
    "            ((x, y), rayon) = cv2.minEnclosingCircle(element)\n",
    "            points.append(np.array([int(x), int(y)]))\n",
    "            break\n",
    "\n",
    "    return points, mask\n",
    "\n",
    "# Function to detect faces using Haar cascade classifier\n",
    "def detect_visage(image):\n",
    "    # Load OpenCV built-in Haar cascade XML\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_alt2.xml\")\n",
    "    \n",
    "    if face_cascade.empty():\n",
    "        print(\"Failed to load Haar cascade XML file.\")\n",
    "        exit()\n",
    "\n",
    "    points = []\n",
    "    rects = []\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    face = face_cascade.detectMultiScale(gray, scaleFactor=1.2, minNeighbors=3)\n",
    "\n",
    "    for x, y, w, h in face:\n",
    "        points.append(np.array([int(x + w / 2), int(y + h / 2)]))\n",
    "        rects.append(np.array([(x, y), (x + w, y + h)]))\n",
    "\n",
    "    return points, rects\n",
    "\n",
    "\n",
    "# Open webcam stream\n",
    "VideoCap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize Kalman Filter with timestep = 0.1s and initial position [10,10]\n",
    "KF = KalmanFilter(0.1, [10, 10])\n",
    "\n",
    "while True:\n",
    "    mask = None\n",
    "    rects = None\n",
    "\n",
    "    ret, frame = VideoCap.read()  # Read frame from webcam\n",
    "\n",
    "    cv2.flip(frame, 1, frame)     # Mirror the frame (optional for user-facing cameras)\n",
    "\n",
    "    # Uncomment if using color range detection:\n",
    "    # points, mask = detect_inrange(frame, 3000, 7000)\n",
    "\n",
    "    points, rects = detect_visage(frame)  # Use face detection instead\n",
    "\n",
    "    etat = KF.predict().astype(np.int32)  # Predict next position with Kalman filter\n",
    "\n",
    "    # Draw predicted position (green)\n",
    "    cv2.circle(frame, (int(etat[0]), int(etat[1])), 2, (0, 255, 0), 5)\n",
    "\n",
    "    # Draw velocity vector (green arrow)\n",
    "    cv2.arrowedLine(\n",
    "        frame,\n",
    "        (int(etat[0]), int(etat[1])),\n",
    "        (int(etat[0] + etat[2]), int(etat[1] + etat[3])),\n",
    "        color=(0, 255, 0),\n",
    "        thickness=3,\n",
    "        tipLength=0.2\n",
    "    )\n",
    "\n",
    "    # If a new measurement is available (face detected)\n",
    "    if len(points) > 0:\n",
    "        KF.update(np.expand_dims(points[0], axis=-1))  # Update Kalman filter\n",
    "        cv2.circle(frame, (points[0][0], points[0][1]), 10, (0, 0, 255), 2)  # Draw measured position (red)\n",
    "\n",
    "    # Draw face rectangle if detected\n",
    "    if rects is not None:\n",
    "        try:\n",
    "            print(rects[0])\n",
    "            cv2.rectangle(frame, rects[0][0], rects[0][1], (0, 0, 255), 1, cv2.LINE_AA)\n",
    "        except:\n",
    "            print(\"erreur\")\n",
    "\n",
    "    # Show the mask (only if color detection was used)\n",
    "    if mask is not None:\n",
    "        cv2.imshow('mask', mask)\n",
    "\n",
    "    # Show the result frame\n",
    "    cv2.imshow('frame', frame)\n",
    "\n",
    "    # Exit on key 'q'\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "VideoCap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''tp 12 : cell phone detection'''\n",
    "import cv2 # manipulate frames and video (display)\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort # to use the model of track \n",
    "from ultralytics import YOLO # to use yolo to track \n",
    "\n",
    "\n",
    "# Set environment variable to avoid duplicate library errors\n",
    "# os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "\n",
    "# Initialize the DeepSort tracker (une instance )\n",
    "''' \n",
    "DeepSort uses :\n",
    "    Kalman Filters & Hungarian Algorithm for { association and prediction }\n",
    "    Neural network for { appearance-based re-identification }\n",
    "It ensures consistent tracking of objects \n",
    "    [even when they temporarily disappear from view or when the camera moves slightly]\n",
    "Each tracked object is assigned\n",
    "    a unique ID (to monitor that object)\n",
    "'''\n",
    "object_tracker = DeepSort()\n",
    "\n",
    "# Initialize the YOLO model (version 8) with a smaller model for faster inference\n",
    "# Use yolov5n for faster inference\n",
    "# DETAILS \n",
    "'''\n",
    "model = YOLO(weights, task=None, mode=None)\n",
    "weights (Required):\n",
    "    path to the custom model weights\n",
    "        Pretrained YOLOv8 weights:\n",
    "            \"yolov8'c'.pt\": c => can be ,n(nano) ,s(small) ,m(medium) ,l(large) ,x(extra large)\n",
    "task (Optional):\n",
    "    Specifies the task the model is meant to perform:\n",
    "        \"detect\": Object detection (default if not specified).\n",
    "        \"segment\": Instance segmentation.\n",
    "        \"classify\": Image classification.\n",
    "mode (Optional):\n",
    "    Specifies the mode of operation:\n",
    "        \"train\": Train a new or custom YOLO model.\n",
    "        \"val\": Validate the performance of the model on a dataset.\n",
    "        \"predict\": Make predictions on images or videos (default).\n",
    "        \"export\": Export the model for inference (e.g., ONNX, TensorRT, etc.).\n",
    "\n",
    "default : \n",
    "- model = YOLO(\"yolov8n-seg.pt/yolov8n.pt\")\n",
    "    Default task : \"segment\".\n",
    "    Default mode: \"predict\".\n",
    "- model = YOLO(\"yolov8n-cls.pt/yolov8n.pt\")\n",
    "    Default task : \"classify\".\n",
    "    Default mode: \"predict\".\n",
    "'''\n",
    "model = YOLO(\"yolo-Weights/yolov5n.pt\")  \n",
    "\n",
    "\n",
    "# code to start the webcam ( 0 => the actual default camera , in my case my laptop)\n",
    "# DETAILS \n",
    "'''\n",
    "cap = cv2.VideoCapture(source, apiPreference)\n",
    "READING of the video frame by frame \n",
    "Parameters:\n",
    "    source (required):\n",
    "        Specifies the video source.\n",
    "        Integer: Refers to the index of the camera.\n",
    "            0: Default camera \n",
    "            1: Second connected camera, and so on.\n",
    "        String: Path to a video file \n",
    "apiPreference (optional):\n",
    "Specifies which API backend to use (e.g., DirectShow, Media Foundation, etc.).\n",
    "Common values include:\n",
    "cv2.CAP_ANY (default): Auto-select the backend.\n",
    "cv2.CAP_DSHOW: DirectShow (Windows).\n",
    "cv2.CAP_AVFOUNDATION: AVFoundation (macOS).\n",
    "'''\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# these lines is to define the capture of cam width and the height \n",
    "# DETAILS \n",
    "'''\n",
    "cap.set(id number ,pixels values )\n",
    "- '3' => CAP_PROP_FRAME_WIDTH the video frame width in pixels.\n",
    "- '4' => CAP_PROP_FRAME_HEIGHT the video frame height in pixels.\n",
    "'''\n",
    "cap.set(3, 640)  \n",
    "cap.set(4, 480)  \n",
    "\n",
    "# Read class names from the model (reads all the classes that are available on the yolo model)\n",
    "# DETAILS\n",
    "'''\n",
    "COCO dataset\n",
    "model.names is a dictionnary : \n",
    "    {\n",
    "        (key: \"value\",)\n",
    "        0: \"person\",\n",
    "        1: \"bicycle\",\n",
    "        2: \"car\",\n",
    "        ...\n",
    "        79: \"toothbrush\"\n",
    "    }\n",
    "'''\n",
    "classNames = model.names  \n",
    "\n",
    "# just take the index of the object we want to detect , in our case , cell phone => 67\n",
    "phone_class_index = 67  \n",
    "\n",
    "# this line ensures that the web cam is indeed opened \n",
    "while cap.isOpened():\n",
    "    '''\n",
    "    cap.read() returns :\n",
    "        success: A boolean  frame was successfully read or not .\n",
    "        img: The actual frame (image) if success is true .\n",
    "    '''\n",
    "    success, img = cap.read()\n",
    "\n",
    "    '''cas frame not read sortir '''\n",
    "    if not success:\n",
    "        break\n",
    "    \n",
    "    # DETAILS\n",
    "    '''\n",
    "    img is the frame we read \n",
    "    the stream value \n",
    "        The stream=True means  'YOLO' will return results as a generator(with streaming)\n",
    "    means :\n",
    "        Instead of returning all detections at once in one frame it returns a set of results that way we can manipulate each as we want , it is also memory saving\n",
    "    '''\n",
    "    '''\n",
    "    here:\n",
    "        preprocessing \n",
    "            resizing the frame to treat it with yolo \n",
    "            normalisation ect \n",
    "        inference \n",
    "            runing cnn of yolo to detect \n",
    "        steaming \n",
    "            already explained \n",
    "    '''\n",
    "    results = model(img, stream=True)\n",
    "\n",
    "    # Prepare a list to store detections for DeepSORT\n",
    "    detections = []\n",
    "\n",
    "    # Process results from YOLO\n",
    "    '''\n",
    "    for each results r we will have : \n",
    "        Bounding Box (r.boxes.xyxy):\n",
    "            (x1, y1, x2, y2).\n",
    "        Class Index (r.boxes.cls):\n",
    "            The index of the detected class \n",
    "        Confidence (r.boxes.conf):\n",
    "            The confidence score for the detection of the class (proba)\n",
    "    '''\n",
    "    # iterate all the results \n",
    "    for r in results:\n",
    "        # retrieve the box detection result of all objects \n",
    "        boxes = r.boxes\n",
    "        # this to only detect a phone \n",
    "        # iterate all the boxes \n",
    "        for box in boxes:\n",
    "            # retrieve the classe of detection \n",
    "            cls = int(box.cls[0])  \n",
    "\n",
    "            # If the detected class is 'cell phone'\n",
    "            '''do traitement , detection + tracking '''\n",
    "            if cls == phone_class_index:\n",
    "                '''get the coordinates'''\n",
    "                x1, y1, x2, y2 = box.xyxy[0]\n",
    "                '''\n",
    "                    float to int\n",
    "                    because : \n",
    "                        Image pixels are discrete and indexed using integers.\n",
    "                '''\n",
    "                x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "\n",
    "                # Calculate the center of the bounding box\n",
    "                center_x = (x1 + x2) // 2\n",
    "                center_y = (y1 + y2) // 2\n",
    "\n",
    "                # Add detection to list for DeepSORT (bbox, confidence, class)\n",
    "                detections.append(([x1, y1, x2, y2], box.conf[0], cls))\n",
    "\n",
    "                # DISPLAY \n",
    "                '''\n",
    "                display rectangle \n",
    "                cv2.rectangle ( frame , the coordinate top left , bottom right , color line rectangle , border thickness)\n",
    "                display a circle \n",
    "                cv2.circle(frame , coordinate of center , radius of circle,color ,circle filled or no (with color))\n",
    "                '''\n",
    "                # Draw bounding box \n",
    "                cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 255), 3)\n",
    "                cv2.circle(img, (center_x, center_y), 5, (0, 255, 0), -1) \n",
    "                ''' \n",
    "                this to display the center x and y (position of the object)\n",
    "                '''\n",
    "                # Display the center position as text on the webcam\n",
    "                center_text = f\"Center: ({center_x}, {center_y})\"\n",
    "                org = (x1, y1 - 10)  \n",
    "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                fontScale = 0.7\n",
    "                color = (0, 255, 0)\n",
    "                thickness = 2\n",
    "                ''' \n",
    "                cv2.putText(frame , text ,where to display (position),font, fontScale, color, thickness)\n",
    "                '''\n",
    "                cv2.putText(img, center_text, org, font, fontScale, color, thickness)\n",
    "\n",
    "\n",
    "    # Pass detections to DeepSORT for tracking\n",
    "    # track the object \n",
    "    ''' \n",
    "    Matching Detections to Existing Tracks:\n",
    "        DeepSort attempts to match the new detections (detections) to previously tracked objects using:\n",
    "            Bounding box overlap (Intersection over Union, IoU).\n",
    "            Appearance features (if enabled).\n",
    "        Updating Tracks:\n",
    "            For matched detections, DeepSort updates the state of the corresponding track .\n",
    "        Creating New Tracks:\n",
    "            If a detection cannot be matched to an existing track, DeepSort adds it.\n",
    "        Removing Lost Tracks:\n",
    "            Tracks that have not been updated for multiple frames => delete.\n",
    "        \n",
    "        Return Value:\n",
    "            tracks: A list of track objects\n",
    "            track_id: ID\n",
    "            to_ltrb(): The bounding box coordinates [left, top, right, bottom] of the tracked object.\n",
    "            is_confirmed(): A flag indicating whether the track is active and confirmed.\n",
    "            Optionally, additional information\n",
    "    '''\n",
    "    tracks = object_tracker.update_tracks(detections, frame=img)\n",
    "\n",
    "    # Draw a moving dot for each track (phone)\n",
    "    for track in tracks:\n",
    "        \n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "        track_id = track.track_id\n",
    "        ltrb = track.to_ltrb()\n",
    "\n",
    "        # Calculate the center of the bounding box for the dot\n",
    "        center_x = int((ltrb[0] + ltrb[2]) // 2)\n",
    "        center_y = int((ltrb[1] + ltrb[3]) // 2)\n",
    "\n",
    "        # Draw the dot at the center of the tracked phone object\n",
    "        cv2.circle(img, (center_x, center_y), 5, (0, 0, 255), -1)  # Red dot for tracking\n",
    "\n",
    "    \n",
    "\n",
    "    # Display the image on webcam with all the added displays \n",
    "    cv2.imshow('Webcam', img)\n",
    "\n",
    "    # press q to quit \n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
